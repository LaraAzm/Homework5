[
  {
    "objectID": "Homework5.html",
    "href": "Homework5.html",
    "title": "ST558 - Homework 5: Models with caret",
    "section": "",
    "text": "#implemented silencing the warnings based on feedback \nlibrary(readr)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(randomForest)"
  },
  {
    "objectID": "Homework5.html#task-1-conceptual-questions",
    "href": "Homework5.html#task-1-conceptual-questions",
    "title": "ST558 - Homework 5: Models with caret",
    "section": "Task 1: Conceptual Questions",
    "text": "Task 1: Conceptual Questions\n\nWhat is the purpose of using cross-validation when fitting a random forest model?\n\n\nCross-validation helps make sure that the model obtains similar results for every branch.\n\n\nDescribe the bagged tree algorithm\n\n\nBagged tree algorithm constructs numerous decision trees of the training data by using random sampling with replacement and averages the resulting predictions which reduces the variance.\n\n\nWhat is meant by a general linear model?\n\n\nIt is simply a generalized form of a lineral model that includes both categorical and continuous variables\n\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\n\nAdding an ineteraction term provides the information regarding predictors that depend on each other. If y cannot happen without x, adding an interaction term will ensure that wholly additive information does not overtake the data.\n\n\nWhy do we split our data into a training and test set?\n\n\nThe idea is to fit competing models on the training set of data and get an idea of how well the model will generalize when applied to the test set. It also saves from doing computation over large data sets."
  },
  {
    "objectID": "Homework5.html#task-2-fitting-models",
    "href": "Homework5.html#task-2-fitting-models",
    "title": "ST558 - Homework 5: Models with caret",
    "section": "Task 2: Fitting Models",
    "text": "Task 2: Fitting Models\n\nQuick EDA/Data Preparation\n\n#reading in the csv \nheart_data &lt;- read_csv(\"./data/heart.csv\")\n\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#checking the format of the data\nspec(heart_data)\n\ncols(\n  Age = col_double(),\n  Sex = col_character(),\n  ChestPainType = col_character(),\n  RestingBP = col_double(),\n  Cholesterol = col_double(),\n  FastingBS = col_double(),\n  RestingECG = col_character(),\n  MaxHR = col_double(),\n  ExerciseAngina = col_character(),\n  Oldpeak = col_double(),\n  ST_Slope = col_character(),\n  HeartDisease = col_double()\n)\n\n#changing the data type for the heart disease column \nheart_data$HeartDisease &lt;- as.factor(heart_data$HeartDisease)\n\n#dropping the ST_Slope variable \nheart_data &lt;- heart_data |&gt;\n              select(!ST_Slope)\n\n#viewing the corrected data\nheart_data\n\n# A tibble: 918 × 11\n     Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1    40 M     ATA                 140         289         0 Normal       172\n 2    49 F     NAP                 160         180         0 Normal       156\n 3    37 M     ATA                 130         283         0 ST            98\n 4    48 F     ASY                 138         214         0 Normal       108\n 5    54 M     NAP                 150         195         0 Normal       122\n 6    39 M     NAP                 120         339         0 Normal       170\n 7    45 F     ATA                 130         237         0 Normal       170\n 8    54 M     ATA                 110         208         0 Normal       142\n 9    37 M     ASY                 140         207         0 Normal       130\n10    48 F     ATA                 120         284         0 Normal       120\n# ℹ 908 more rows\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, HeartDisease &lt;fct&gt;\n\n\n\nData prep for kNN through adding dummy columns\n\n#creating a list of the variables to be run through the `dummyVars` function\ndummy_list &lt;- c(\"ChestPainType\", \"RestingECG\", \"ExerciseAngina\")\n\n#creating a temporary data set for the dummy function \nheart_dummy &lt;- heart_data |&gt;\n               select(ChestPainType, RestingECG, ExerciseAngina)\n\n#dummyfying the data \nheart_dummy &lt;- dummyVars(\" ~ .\", data = heart_dummy)\n\n#using predict to create new columns \nheart_predict &lt;- as_tibble(data.frame(predict(heart_dummy, newdata = heart_data)))\n\n#adjusting the column type for later use\nheart_predict &lt;- heart_predict |&gt;\n                 mutate(across(where(is.double), as.factor))\n\n\n#combining the data into one object and dropping the character variables\nheart_data2 &lt;- bind_cols(heart_data, heart_predict)\n\nheart_data2 &lt;- heart_data2 |&gt;\n               select(!all_of(dummy_list))\n           \nheart_data2\n\n# A tibble: 918 × 17\n     Age Sex   RestingBP Cholesterol FastingBS MaxHR Oldpeak HeartDisease\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;       \n 1    40 M           140         289         0   172     0   0           \n 2    49 F           160         180         0   156     1   1           \n 3    37 M           130         283         0    98     0   0           \n 4    48 F           138         214         0   108     1.5 1           \n 5    54 M           150         195         0   122     0   0           \n 6    39 M           120         339         0   170     0   0           \n 7    45 F           130         237         0   170     0   0           \n 8    54 M           110         208         0   142     0   0           \n 9    37 M           140         207         0   130     1.5 1           \n10    48 F           120         284         0   120     0   0           \n# ℹ 908 more rows\n# ℹ 9 more variables: ChestPainTypeASY &lt;fct&gt;, ChestPainTypeATA &lt;fct&gt;,\n#   ChestPainTypeNAP &lt;fct&gt;, ChestPainTypeTA &lt;fct&gt;, RestingECGLVH &lt;fct&gt;,\n#   RestingECGNormal &lt;fct&gt;, RestingECGST &lt;fct&gt;, ExerciseAnginaN &lt;fct&gt;,\n#   ExerciseAnginaY &lt;fct&gt;\n\n\n\n\nSplitting the data\n\n#setting up to split the data into two for later use as training and testing \ntrain &lt;- sample(1:nrow(heart_data), size = nrow(heart_data)*0.6)\ntest &lt;- setdiff(1:nrow(heart_data), train)\n\n#subsetting the data set\nheart_train &lt;- heart_data[train, ]\nheart_test &lt;- heart_data[test, ]\n\n#repeating with dummy data\ndummy_train &lt;- heart_data2[train, ]\ndummy_test &lt;- heart_data2[test, ]\n\n\n\nQuick EDA\n\n#quick count of different variables \nheart_data |&gt;\n  count(HeartDisease, ChestPainType, Sex)\n\n# A tibble: 16 × 4\n   HeartDisease ChestPainType Sex       n\n   &lt;fct&gt;        &lt;chr&gt;         &lt;chr&gt; &lt;int&gt;\n 1 0            ASY           F        31\n 2 0            ASY           M        73\n 3 0            ATA           F        56\n 4 0            ATA           M        93\n 5 0            NAP           F        47\n 6 0            NAP           M        84\n 7 0            TA            F         9\n 8 0            TA            M        17\n 9 1            ASY           F        39\n10 1            ASY           M       353\n11 1            ATA           F         4\n12 1            ATA           M        20\n13 1            NAP           F         6\n14 1            NAP           M        66\n15 1            TA            F         1\n16 1            TA            M        19\n\n#summarizing the data\nsummary(heart_data)\n\n      Age            Sex            ChestPainType        RestingBP    \n Min.   :28.00   Length:918         Length:918         Min.   :  0.0  \n 1st Qu.:47.00   Class :character   Class :character   1st Qu.:120.0  \n Median :54.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :53.51                                         Mean   :132.4  \n 3rd Qu.:60.00                                         3rd Qu.:140.0  \n Max.   :77.00                                         Max.   :200.0  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   :  0.0   Min.   :0.0000   Length:918         Min.   : 60.0  \n 1st Qu.:173.2   1st Qu.:0.0000   Class :character   1st Qu.:120.0  \n Median :223.0   Median :0.0000   Mode  :character   Median :138.0  \n Mean   :198.8   Mean   :0.2331                      Mean   :136.8  \n 3rd Qu.:267.0   3rd Qu.:0.0000                      3rd Qu.:156.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak        HeartDisease\n Length:918         Min.   :-2.6000   0:410       \n Class :character   1st Qu.: 0.0000   1:508       \n Mode  :character   Median : 0.6000               \n                    Mean   : 0.8874               \n                    3rd Qu.: 1.5000               \n                    Max.   : 6.2000               \n\n#plotting resting heart rate against heart disease\nggplot(data = heart_data, mapping = aes(x = RestingBP, colour = HeartDisease)) +\n   geom_histogram(binwidth = 0.1)\n\n\n\n\n\n\n\n#plotting resting age against heart diseases \nggplot(heart_data, aes(Age, fill = HeartDisease)) +\n   geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n#plotting resting age against heart max heart rate \nggplot(heart_data, aes(Cholesterol, MaxHR)) +\n  geom_smooth(method = \"lm\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#summarizing based on whether someone has heart disease at a given age\ntable(heart_data$HeartDisease, heart_data$Age)\n\n   \n    28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52\n  0  1  3  1  1  3  1  5  7  4 10  5 13  7 18 15 13 13 15 11  8 16 10 11 20 17\n  1  0  0  0  1  2  1  2  4  2  1 11  2  6  6  3 11  6  3 13 11 15 11 14 15 19\n   \n    53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77\n  0 18 28 17 13 12 14 12  8  5 10  7  7  6  6  3  4  3  1  3  1  0  2  1  1  0\n  1 15 23 24 25 26 28 23 24 26 25 23 15 15  7 12  6 10  6  2  3  1  5  2  1  2\n\n\n\n\n\nkNN\n\n#setting up the grid to be used later in `tuneGrid`\nk &lt;- expand.grid(k = 1:40)\n\n#setting up the \ntrainctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nset.seed(478)\n\n#training the kNN model\nknn_fit &lt;- train(ChestPainTypeASY ~., \n                 data = dummy_train, \n                 method = \"knn\",\n                 preProcess = c(\"center\", \"scale\"),\n                 trControl=trainctrl,\n                 tuneGrid = k,\n                 tuneLength = 10)\n\n\n#running the fit\nknn_fit\n\nk-Nearest Neighbors \n\n550 samples\n 16 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 494, 495, 495, 495, 496, 495, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.9593751  0.9189075\n   2  0.9490917  0.8983752\n   3  0.9484420  0.8970910\n   4  0.9399331  0.8801097\n   5  0.9369248  0.8742180\n   6  0.9320755  0.8645335\n   7  0.9326820  0.8657532\n   8  0.9241619  0.8487714\n   9  0.9211091  0.8426055\n  10  0.9144092  0.8293394\n  11  0.9162494  0.8330115\n  12  0.9162831  0.8331222\n  13  0.9144649  0.8295658\n  14  0.9163167  0.8332498\n  15  0.9169557  0.8345570\n  16  0.9139250  0.8284998\n  17  0.9127012  0.8260749\n  18  0.9115111  0.8237257\n  19  0.9114671  0.8236710\n  20  0.9084359  0.8175869\n  21  0.9090753  0.8188974\n  22  0.9066394  0.8140436\n  23  0.9090645  0.8188579\n  24  0.9084584  0.8176509\n  25  0.9078744  0.8164568\n  26  0.9054056  0.8115171\n  27  0.9048325  0.8103963\n  28  0.9042264  0.8092113\n  29  0.9030026  0.8067541\n  30  0.9023741  0.8055598\n  31  0.9035646  0.8078822\n  32  0.8980980  0.7969510\n  33  0.8962795  0.7932709\n  34  0.8981093  0.7969395\n  35  0.9011400  0.8029388\n  36  0.8999278  0.8005471\n  37  0.8999058  0.8004645\n  38  0.9005123  0.8016526\n  39  0.8998842  0.8004218\n  40  0.8998729  0.8003560\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 1.\n\n\n\n#running the model using test data\nknn_pred &lt;- predict(knn_fit, newdata = dummy_test)\n\n#checking the accuracy of the sample results\nknn_pred2 &lt;- postResample(predict(knn_fit, newdata = dummy_test), dummy_test$ChestPainTypeASY)\n\n\n#checking how well the chosen model does on the test data set\nconfusionMatrix(knn_pred, dummy_test$ChestPainTypeASY)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 135   1\n         1   4 228\n                                          \n               Accuracy : 0.9864          \n                 95% CI : (0.9686, 0.9956)\n    No Information Rate : 0.6223          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.971           \n                                          \n Mcnemar's Test P-Value : 0.3711          \n                                          \n            Sensitivity : 0.9712          \n            Specificity : 0.9956          \n         Pos Pred Value : 0.9926          \n         Neg Pred Value : 0.9828          \n             Prevalence : 0.3777          \n         Detection Rate : 0.3668          \n   Detection Prevalence : 0.3696          \n      Balanced Accuracy : 0.9834          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nLogistic Regression\n\nSetting up logistic regression models\n\n#model 1 - using just the heart disease and chest pain type predictors\nglm_mod1 &lt;- train(HeartDisease ~ ChestPainType,\n  data = heart_train,\n  method = \"glm\",\n  family = binomial,\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainctrl)\n\n#model 2 - using just the heart disease and whether pain while exercising exists\nglm_mod2 &lt;- train(HeartDisease ~ ExerciseAngina,\n  data = heart_train,\n  method = \"glm\",\n  family = binomial,\n  preProcess = c(\"center\", \"scale\"), \n  trControl = trainctrl)\n\n#model 3 - combines model of previous the variables\nglm_mod3 &lt;- train(HeartDisease ~ ChestPainType + ExerciseAngina,\n  data = heart_train,\n  method = \"glm\",\n  family = binomial,\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainctrl)\n\n\n\nResults\n\n#showing results in a combined set ast shown in the notes\nrbind(c(\"Mod1\", glm_mod1$results[c(\"Accuracy\", \"Kappa\")]),\n      c(\"Mod2\", glm_mod2$results[c(\"Accuracy\", \"Kappa\")]),\n      c(\"Mod3\", glm_mod3$results[c(\"Accuracy\", \"Kappa\")])\n      )\n\n            Accuracy  Kappa    \n[1,] \"Mod1\" 0.7564446 0.5141922\n[2,] \"Mod2\" 0.7382644 0.4839039\n[3,] \"Mod3\" 0.78252   0.5614179\n\n#model 3 is the most accurate one\nsummary(glm_mod3)\n\n\nCall:\nNULL\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.23806    0.11360   2.096   0.0361 *  \nChestPainTypeATA -0.96512    0.12554  -7.688 1.50e-14 ***\nChestPainTypeNAP -0.75134    0.11174  -6.724 1.77e-11 ***\nChestPainTypeTA  -0.22271    0.09591  -2.322   0.0202 *  \nExerciseAnginaY   0.97976    0.12097   8.099 5.53e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 760.60  on 549  degrees of freedom\nResidual deviance: 520.32  on 545  degrees of freedom\nAIC: 530.32\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nRunning model 3 on test set\n\n#running the model using test data\nglm_pred &lt;- predict(glm_mod3, newdata = heart_test)\n\n#checking the accuracy of the sample results\nglm_pred2 &lt;- postResample(predict(glm_mod3, newdata = heart_test), heart_test$HeartDisease)\nglm_pred2\n\n Accuracy     Kappa \n0.7798913 0.5285335 \n\n\n\n#checking how well the chosen model does on the test data set\nconfusionMatrix(glm_pred, heart_test$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  93  23\n         1  58 194\n                                         \n               Accuracy : 0.7799         \n                 95% CI : (0.734, 0.8212)\n    No Information Rate : 0.5897         \n    P-Value [Acc &gt; NIR] : 9.632e-15      \n                                         \n                  Kappa : 0.5285         \n                                         \n Mcnemar's Test P-Value : 0.0001582      \n                                         \n            Sensitivity : 0.6159         \n            Specificity : 0.8940         \n         Pos Pred Value : 0.8017         \n         Neg Pred Value : 0.7698         \n             Prevalence : 0.4103         \n         Detection Rate : 0.2527         \n   Detection Prevalence : 0.3152         \n      Balanced Accuracy : 0.7550         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\n\n\n\nTree Models\n\nClassification tree model\n\n#setting up the cp sequence \ncp &lt;- expand.grid(cp = seq(0, 0.1, 0.001))\n\n#building the model with classification tree model and training it on the train data set\nclass_tree &lt;- train(HeartDisease ~ Age + Sex + MaxHR + ExerciseAngina, \n                 data = heart_train, \n                 method = \"rpart\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = cp,\n                 trControl = trainctrl)\n\n#running the model\nclass_tree\n\nCART \n\n550 samples\n  4 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (4), scaled (4) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 495, 495, 495, 495, 495, 495, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7278275  0.4529211\n  0.001  0.7278275  0.4529211\n  0.002  0.7290396  0.4558603\n  0.003  0.7326647  0.4628792\n  0.004  0.7387478  0.4759550\n  0.005  0.7332816  0.4648439\n  0.006  0.7344829  0.4679244\n  0.007  0.7460650  0.4892439\n  0.008  0.7479056  0.4933282\n  0.009  0.7520611  0.5009525\n  0.010  0.7520611  0.5009525\n  0.011  0.7520503  0.5010354\n  0.012  0.7520503  0.5010354\n  0.013  0.7593458  0.5164671\n  0.014  0.7593458  0.5164671\n  0.015  0.7593458  0.5164671\n  0.016  0.7593458  0.5164671\n  0.017  0.7593458  0.5164671\n  0.018  0.7611091  0.5202114\n  0.019  0.7611091  0.5202114\n  0.020  0.7611091  0.5202114\n  0.021  0.7611091  0.5202114\n  0.022  0.7611091  0.5202114\n  0.023  0.7611091  0.5202114\n  0.024  0.7611091  0.5202114\n  0.025  0.7598745  0.5180007\n  0.026  0.7622988  0.5230207\n  0.027  0.7622988  0.5230207\n  0.028  0.7550910  0.5097536\n  0.029  0.7550910  0.5097536\n  0.030  0.7550910  0.5097536\n  0.031  0.7490304  0.4984666\n  0.032  0.7490304  0.4984666\n  0.033  0.7466061  0.4938851\n  0.034  0.7466061  0.4938851\n  0.035  0.7393334  0.4808021\n  0.036  0.7393334  0.4808021\n  0.037  0.7308478  0.4658798\n  0.038  0.7308478  0.4658798\n  0.039  0.7302525  0.4657150\n  0.040  0.7302525  0.4657150\n  0.041  0.7302525  0.4664459\n  0.042  0.7302525  0.4664459\n  0.043  0.7314646  0.4694475\n  0.044  0.7314646  0.4694475\n  0.045  0.7314646  0.4694475\n  0.046  0.7338889  0.4749315\n  0.047  0.7338889  0.4749315\n  0.048  0.7357071  0.4790083\n  0.049  0.7357071  0.4790083\n  0.050  0.7381313  0.4840601\n  0.051  0.7381313  0.4840601\n  0.052  0.7381313  0.4840601\n  0.053  0.7381313  0.4840601\n  0.054  0.7381313  0.4840601\n  0.055  0.7381313  0.4840601\n  0.056  0.7381313  0.4840601\n  0.057  0.7381313  0.4840601\n  0.058  0.7381313  0.4840601\n  0.059  0.7381313  0.4840601\n  0.060  0.7381313  0.4840601\n  0.061  0.7381313  0.4840601\n  0.062  0.7381313  0.4840601\n  0.063  0.7381313  0.4840601\n  0.064  0.7381313  0.4840601\n  0.065  0.7381313  0.4840601\n  0.066  0.7381313  0.4840601\n  0.067  0.7381313  0.4840601\n  0.068  0.7381313  0.4840601\n  0.069  0.7381313  0.4840601\n  0.070  0.7381313  0.4840601\n  0.071  0.7381313  0.4840601\n  0.072  0.7381313  0.4840601\n  0.073  0.7381313  0.4840601\n  0.074  0.7381313  0.4840601\n  0.075  0.7381313  0.4840601\n  0.076  0.7381313  0.4840601\n  0.077  0.7381313  0.4840601\n  0.078  0.7381313  0.4840601\n  0.079  0.7381313  0.4840601\n  0.080  0.7381313  0.4840601\n  0.081  0.7381313  0.4840601\n  0.082  0.7381313  0.4840601\n  0.083  0.7381313  0.4840601\n  0.084  0.7381313  0.4840601\n  0.085  0.7381313  0.4840601\n  0.086  0.7381313  0.4840601\n  0.087  0.7381313  0.4840601\n  0.088  0.7381313  0.4840601\n  0.089  0.7381313  0.4840601\n  0.090  0.7381313  0.4840601\n  0.091  0.7381313  0.4840601\n  0.092  0.7381313  0.4840601\n  0.093  0.7381313  0.4840601\n  0.094  0.7381313  0.4840601\n  0.095  0.7381313  0.4840601\n  0.096  0.7381313  0.4840601\n  0.097  0.7381313  0.4840601\n  0.098  0.7381313  0.4840601\n  0.099  0.7381313  0.4840601\n  0.100  0.7381313  0.4840601\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.027.\n\n\n\n\nRandom forest\n\n#setting up mtry sequence with 11 as the number of predictors\nmtry &lt;- expand.grid(mtry = seq(1:11))\n\n#building the model with random forest model and training it on the train data set\nrand_tree &lt;- train(HeartDisease ~ ., \n                 data = heart_train, \n                 method = \"rf\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtry,\n                 trControl = trainctrl)\n\n#running the model\nrand_tree\n\nRandom Forest \n\n550 samples\n 10 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (13), scaled (13) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 495, 496, 495, 495, 495, 495, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   1    0.8308899  0.6612402\n   2    0.8369072  0.6735014\n   3    0.8320587  0.6634809\n   4    0.8332600  0.6655145\n   5    0.8296128  0.6582097\n   6    0.8314310  0.6619248\n   7    0.8308245  0.6607634\n   8    0.8259540  0.6510160\n   9    0.8229565  0.6449141\n  10    0.8217332  0.6426136\n  11    0.8205323  0.6402419\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\n\n\nBoosted tree\n\n#setting up a grid with defined specifications\nmtryGrid &lt;- expand.grid(interaction.depth = c(1, 2, 3),\n                    #not sure how this was meant to be interpreted so kept it as whole numbers rather than a sequence\n                    n.trees = c(25, 50, 100, 200),\n                    n.minobsinnode = 10,\n                    shrinkage = 0.1)\n\n#building the model with boosted tree model and training it on the train data set\nboost_tree &lt;- train(HeartDisease ~ ., \n                 data = heart_train, \n                 method = \"gbm\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtryGrid,\n                 trControl = trainctrl, \n                 verbose = FALSE)\n\n#running the model\nboost_tree\n\nStochastic Gradient Boosting \n\n550 samples\n 10 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (13), scaled (13) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 495, 496, 495, 495, 495, 494, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.8114458  0.6221194\n  1                   50      0.8187197  0.6372891\n  1                  100      0.8326495  0.6648916\n  1                  200      0.8338384  0.6672768\n  2                   25      0.8272387  0.6532866\n  2                   50      0.8375080  0.6743029\n  2                  100      0.8429842  0.6851837\n  2                  200      0.8356786  0.6705494\n  3                   25      0.8350176  0.6689619\n  3                   50      0.8380816  0.6753681\n  3                  100      0.8423573  0.6838614\n  3                  200      0.8326166  0.6640683\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 100, interaction.depth =\n 2, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\n\n\nApplying to test data\n\n#running the built models over testing data\nclass_pred &lt;- predict(class_tree, newdata = heart_test)\n\nrand_pred &lt;- predict(rand_tree, newdata = heart_test)\n\nboost_pred &lt;- predict(boost_tree, newdata = heart_test)\n\n\n#running the test data with confusion matrix\nconfusionMatrix(class_pred, heart_test$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 112  51\n         1  39 166\n                                          \n               Accuracy : 0.7554          \n                 95% CI : (0.7082, 0.7985)\n    No Information Rate : 0.5897          \n    P-Value [Acc &gt; NIR] : 1.852e-11       \n                                          \n                  Kappa : 0.5006          \n                                          \n Mcnemar's Test P-Value : 0.2463          \n                                          \n            Sensitivity : 0.7417          \n            Specificity : 0.7650          \n         Pos Pred Value : 0.6871          \n         Neg Pred Value : 0.8098          \n             Prevalence : 0.4103          \n         Detection Rate : 0.3043          \n   Detection Prevalence : 0.4429          \n      Balanced Accuracy : 0.7533          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n#running the test data with confusion matrix\nconfusionMatrix(rand_pred, heart_test$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 121  29\n         1  30 188\n                                          \n               Accuracy : 0.8397          \n                 95% CI : (0.7981, 0.8757)\n    No Information Rate : 0.5897          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6684          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8013          \n            Specificity : 0.8664          \n         Pos Pred Value : 0.8067          \n         Neg Pred Value : 0.8624          \n             Prevalence : 0.4103          \n         Detection Rate : 0.3288          \n   Detection Prevalence : 0.4076          \n      Balanced Accuracy : 0.8338          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n#running the test data with confusion matrix\nconfusionMatrix(boost_pred, heart_test$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 119  34\n         1  32 183\n                                          \n               Accuracy : 0.8207          \n                 95% CI : (0.7776, 0.8585)\n    No Information Rate : 0.5897          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6301          \n                                          \n Mcnemar's Test P-Value : 0.902           \n                                          \n            Sensitivity : 0.7881          \n            Specificity : 0.8433          \n         Pos Pred Value : 0.7778          \n         Neg Pred Value : 0.8512          \n             Prevalence : 0.4103          \n         Detection Rate : 0.3234          \n   Detection Prevalence : 0.4158          \n      Balanced Accuracy : 0.8157          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\n\nWrap up\n\nWhen comparing the results, the accuracy of the kNN test data is hands down the best out of all the other methods."
  }
]